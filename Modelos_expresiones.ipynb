{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imagenes/bannermodelado_cnn.jpg\" alt=\"Imagen creada con inteligencia artificial y editada con Microsoft Paint\" style=\"border-radius: 15px; width: 95%;\">\n",
    "\n",
    "*Imagen creada con inteligencia artificial*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BIBLIOTECAS USADAS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report \n",
    "import os\n",
    "from keras.models import save_model \n",
    "from tensorflow.keras.applications import VGG16  \n",
    "from tensorflow.keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CARGA DEL CONJUNTO DE DATOS 'FER-2013'** \n",
    ">**Realmente no cargamos el dataset 'fer2013' tal cual; cargamos un dataset obtenido tras aplicar técnicas de data augmentation. En el Jupyter Notebook 'Data_augmentation_fer2013' se realiza y explica el proceso.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset ha sido cargado correctamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datos/fer2013/fer2013_blc_todos_rotados.csv')\n",
    "print(\"El dataset ha sido cargado correctamente.\") \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREPARACIÓN PREVIA AL SPLIT**  \n",
    ">>**La columna 'Usage' no nos es útil. Le ponemos cara de asco y la borramos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Usage'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SEPARAMOS \"X\" E \"y\" PIXELES Y ETIQUETAS.** \n",
    "> **La columna 'emotion' contiene las etiquetas, y la columna 'pixels' contiene las cadenas de números que dan lugar a las fotografías.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['emotion'])  \n",
    "y = df['emotion']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONVERSIÓN A ARRAY Y NORMALIZACIÓN DE LA COLUMNA 'PIXELS'**  \n",
    ">**Los modelos, al menos con los que trabajaremos, necesitan arrays de NumPy; no pueden trabajar con cadenas. Además, deben estar normalizados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del array X_train_pixels: (125846, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "def string_to_image_array(string):\n",
    "    pixels = np.array(string.split(), dtype=np.float32)\n",
    "    return pixels.reshape((48, 48, 1))\n",
    "\n",
    "X_pixels = np.array([string_to_image_array(pixels) for pixels in X['pixels']])\n",
    "X_pixels = X_pixels.astype('float32')\n",
    "X_pixels /= 255.0 \n",
    "\n",
    "print(\"Shape del array X_train_pixels:\", X_pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SPLIT**  \n",
    ">**Dividiremos nuestro dataset en tres partes: train, validación y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Se han separado de forma balanceada?\n",
      "Tanto por 1 de clases en y_train:\n",
      "emotion\n",
      "1    0.142864\n",
      "0    0.142864\n",
      "3    0.142854\n",
      "6    0.142854\n",
      "4    0.142854\n",
      "5    0.142854\n",
      "2    0.142854\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tanto por 1 de clases en y_val:\n",
      "emotion\n",
      "2    0.142868\n",
      "6    0.142868\n",
      "5    0.142868\n",
      "3    0.142868\n",
      "0    0.142868\n",
      "4    0.142868\n",
      "1    0.142789\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tanto por 1 de clases en y_test:\n",
      "emotion\n",
      "1    0.142868\n",
      "2    0.142868\n",
      "3    0.142868\n",
      "4    0.142868\n",
      "5    0.142868\n",
      "6    0.142868\n",
      "0    0.142789\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_pixels, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "print(\"¿Se han separado de forma balanceada?\")\n",
    "print(\"Tanto por 1 de clases en y_train:\")\n",
    "print(y_train.value_counts(True))\n",
    "\n",
    "print(\"\\nTanto por 1 de clases en y_val:\")\n",
    "print(y_val.value_counts(True))\n",
    "\n",
    "print(\"\\nTanto por 1 de clases en y_test:\")\n",
    "print(y_test.value_counts(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Sí, la estratificación ha sido correcta. El porcentaje de cada clase es muy similar en train, validación y test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODELOS:**  \n",
    ">**En primer lugar, definiremos, entrenaremos y evaluaremos un modelo de Red Neuronal Convolucional (CNN). En segundo lugar, haremos lo mismo con un modelo de Transfer Learning basado en VGG16.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEFINICIÓN DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pepea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "\n",
    "# Primera capa convolucional\n",
    "model_cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1))) #El modelo trabaja con arrays de 48x48 de una sola dimension, o sea capas... En blanco y negro, hablando en plata.\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Segunda capa convolucional\n",
    "model_cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Tercera capa convolucional\n",
    "model_cnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Cuarta capa convolucional\n",
    "model_cnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capa de flatten para conectar con la capa densa\n",
    "model_cnn.add(Flatten())\n",
    "\n",
    "# Quinta capa densa (totalmente conectada)\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))  # Dropout para reducir overfitting\n",
    "\n",
    "# Sexta capa densa (totalmente conectada)\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))  # Dropout para reducir overfitting\n",
    "\n",
    "# Capa de salida (no se cambia)\n",
    "model_cnn.add(Dense(7, activation='softmax'))  # Capa de salida con 7 clases (expresiones faciales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COMPILADO DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(loss='sparse_categorical_crossentropy',  \n",
    "              optimizer=Adam(learning_rate=0.0002),  #He ido probando, este ha dado muy buen resultado\n",
    "              metrics=['accuracy'])  # Buscamos la mayor presicion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CALLBACK PARA EL MODELO CNN**   \n",
    ">**Para detener el entrenamiento si la mejora en la métrica se detiene.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=40, restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**El hiperparámetro 'patience' se llama así por algo, es decir... si aumentas 'patience', el modelo tendrá más paciencia, seguirá durante más épocas sin ver mejoras (a la espera de que mejore en otra o se acaben las épocas).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ENTRENAMIENTO DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 132ms/step - accuracy: 0.2086 - loss: 1.8715 - val_accuracy: 0.4545 - val_loss: 1.4219\n",
      "Epoch 2/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 135ms/step - accuracy: 0.4758 - loss: 1.3990 - val_accuracy: 0.5593 - val_loss: 1.1799\n",
      "Epoch 3/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 130ms/step - accuracy: 0.5653 - loss: 1.1676 - val_accuracy: 0.6124 - val_loss: 1.0382\n",
      "Epoch 4/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.6221 - loss: 1.0113 - val_accuracy: 0.6411 - val_loss: 0.9413\n",
      "Epoch 5/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 126ms/step - accuracy: 0.6646 - loss: 0.9029 - val_accuracy: 0.6610 - val_loss: 0.8988\n",
      "Epoch 6/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 126ms/step - accuracy: 0.6956 - loss: 0.8176 - val_accuracy: 0.6718 - val_loss: 0.8646\n",
      "Epoch 7/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 127ms/step - accuracy: 0.7317 - loss: 0.7245 - val_accuracy: 0.6888 - val_loss: 0.8246\n",
      "Epoch 8/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.7647 - loss: 0.6446 - val_accuracy: 0.7052 - val_loss: 0.8139\n",
      "Epoch 9/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.7982 - loss: 0.5612 - val_accuracy: 0.7046 - val_loss: 0.8346\n",
      "Epoch 10/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.8298 - loss: 0.4788 - val_accuracy: 0.7275 - val_loss: 0.7691\n",
      "Epoch 11/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.8592 - loss: 0.4026 - val_accuracy: 0.7311 - val_loss: 0.7939\n",
      "Epoch 12/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 123ms/step - accuracy: 0.8796 - loss: 0.3459 - val_accuracy: 0.7443 - val_loss: 0.8088\n",
      "Epoch 13/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 123ms/step - accuracy: 0.8993 - loss: 0.2938 - val_accuracy: 0.7499 - val_loss: 0.8141\n",
      "Epoch 14/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 123ms/step - accuracy: 0.9145 - loss: 0.2462 - val_accuracy: 0.7608 - val_loss: 0.8142\n",
      "Epoch 15/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 123ms/step - accuracy: 0.9283 - loss: 0.2138 - val_accuracy: 0.7599 - val_loss: 0.8552\n",
      "Epoch 16/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9390 - loss: 0.1831 - val_accuracy: 0.7650 - val_loss: 0.9153\n",
      "Epoch 17/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9466 - loss: 0.1618 - val_accuracy: 0.7627 - val_loss: 0.9173\n",
      "Epoch 18/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9511 - loss: 0.1477 - val_accuracy: 0.7692 - val_loss: 0.9383\n",
      "Epoch 19/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9576 - loss: 0.1285 - val_accuracy: 0.7683 - val_loss: 0.9643\n",
      "Epoch 20/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9610 - loss: 0.1166 - val_accuracy: 0.7770 - val_loss: 0.9681\n",
      "Epoch 21/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.9652 - loss: 0.1070 - val_accuracy: 0.7650 - val_loss: 1.0165\n",
      "Epoch 22/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.9671 - loss: 0.1058 - val_accuracy: 0.7739 - val_loss: 0.9934\n",
      "Epoch 23/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.9689 - loss: 0.0955 - val_accuracy: 0.7708 - val_loss: 0.9857\n",
      "Epoch 24/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 127ms/step - accuracy: 0.9724 - loss: 0.0853 - val_accuracy: 0.7681 - val_loss: 1.0764\n",
      "Epoch 25/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 127ms/step - accuracy: 0.9735 - loss: 0.0811 - val_accuracy: 0.7854 - val_loss: 1.0062\n",
      "Epoch 26/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.9749 - loss: 0.0771 - val_accuracy: 0.7672 - val_loss: 1.1106\n",
      "Epoch 27/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 124ms/step - accuracy: 0.9756 - loss: 0.0755 - val_accuracy: 0.7772 - val_loss: 1.1046\n",
      "Epoch 28/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9778 - loss: 0.0701 - val_accuracy: 0.7777 - val_loss: 1.1000\n",
      "Epoch 29/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 124ms/step - accuracy: 0.9784 - loss: 0.0660 - val_accuracy: 0.7897 - val_loss: 1.0095\n",
      "Epoch 30/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9790 - loss: 0.0670 - val_accuracy: 0.7818 - val_loss: 1.1497\n",
      "Epoch 31/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 124ms/step - accuracy: 0.9799 - loss: 0.0634 - val_accuracy: 0.7776 - val_loss: 1.1684\n",
      "Epoch 32/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9812 - loss: 0.0593 - val_accuracy: 0.7707 - val_loss: 1.1800\n",
      "Epoch 33/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 125ms/step - accuracy: 0.9820 - loss: 0.0584 - val_accuracy: 0.7853 - val_loss: 1.1035\n",
      "Epoch 34/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9826 - loss: 0.0534 - val_accuracy: 0.7852 - val_loss: 1.1058\n",
      "Epoch 35/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.9833 - loss: 0.0497 - val_accuracy: 0.7860 - val_loss: 1.1394\n",
      "Epoch 36/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9818 - loss: 0.0557 - val_accuracy: 0.7836 - val_loss: 1.1673\n",
      "Epoch 37/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9843 - loss: 0.0506 - val_accuracy: 0.7919 - val_loss: 1.1433\n",
      "Epoch 38/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 125ms/step - accuracy: 0.9839 - loss: 0.0479 - val_accuracy: 0.7851 - val_loss: 1.1828\n",
      "Epoch 39/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9845 - loss: 0.0491 - val_accuracy: 0.7885 - val_loss: 1.2766\n",
      "Epoch 40/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9856 - loss: 0.0432 - val_accuracy: 0.7865 - val_loss: 1.2260\n",
      "Epoch 41/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9846 - loss: 0.0469 - val_accuracy: 0.7874 - val_loss: 1.2814\n",
      "Epoch 42/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9851 - loss: 0.0459 - val_accuracy: 0.7860 - val_loss: 1.3282\n",
      "Epoch 43/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9854 - loss: 0.0450 - val_accuracy: 0.7877 - val_loss: 1.2442\n",
      "Epoch 44/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9875 - loss: 0.0388 - val_accuracy: 0.7851 - val_loss: 1.3001\n",
      "Epoch 45/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9853 - loss: 0.0440 - val_accuracy: 0.7814 - val_loss: 1.3145\n",
      "Epoch 46/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9856 - loss: 0.0442 - val_accuracy: 0.7868 - val_loss: 1.3002\n",
      "Epoch 47/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 136ms/step - accuracy: 0.9873 - loss: 0.0387 - val_accuracy: 0.7898 - val_loss: 1.2733\n",
      "Epoch 48/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 164ms/step - accuracy: 0.9869 - loss: 0.0392 - val_accuracy: 0.7943 - val_loss: 1.2633\n",
      "Epoch 49/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 138ms/step - accuracy: 0.9871 - loss: 0.0418 - val_accuracy: 0.7882 - val_loss: 1.2836\n",
      "Epoch 50/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 130ms/step - accuracy: 0.9879 - loss: 0.0367 - val_accuracy: 0.7865 - val_loss: 1.3387\n",
      "Epoch 51/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 129ms/step - accuracy: 0.9876 - loss: 0.0375 - val_accuracy: 0.7851 - val_loss: 1.2547\n",
      "Epoch 52/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 129ms/step - accuracy: 0.9880 - loss: 0.0376 - val_accuracy: 0.7915 - val_loss: 1.3295\n",
      "Epoch 53/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 129ms/step - accuracy: 0.9887 - loss: 0.0342 - val_accuracy: 0.7932 - val_loss: 1.2954\n",
      "Epoch 54/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 130ms/step - accuracy: 0.9891 - loss: 0.0325 - val_accuracy: 0.7886 - val_loss: 1.3363\n",
      "Epoch 55/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 129ms/step - accuracy: 0.9882 - loss: 0.0360 - val_accuracy: 0.7871 - val_loss: 1.3439\n",
      "Epoch 56/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 129ms/step - accuracy: 0.9892 - loss: 0.0347 - val_accuracy: 0.7898 - val_loss: 1.3886\n",
      "Epoch 57/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 130ms/step - accuracy: 0.9895 - loss: 0.0317 - val_accuracy: 0.7896 - val_loss: 1.3515\n",
      "Epoch 58/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 131ms/step - accuracy: 0.9876 - loss: 0.0386 - val_accuracy: 0.7971 - val_loss: 1.3867\n",
      "Epoch 59/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 132ms/step - accuracy: 0.9881 - loss: 0.0375 - val_accuracy: 0.7936 - val_loss: 1.3109\n",
      "Epoch 60/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 134ms/step - accuracy: 0.9907 - loss: 0.0274 - val_accuracy: 0.7945 - val_loss: 1.3478\n",
      "Epoch 61/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 135ms/step - accuracy: 0.9901 - loss: 0.0275 - val_accuracy: 0.7835 - val_loss: 1.3871\n",
      "Epoch 62/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 136ms/step - accuracy: 0.9874 - loss: 0.0415 - val_accuracy: 0.7924 - val_loss: 1.4396\n",
      "Epoch 63/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 137ms/step - accuracy: 0.9901 - loss: 0.0308 - val_accuracy: 0.7961 - val_loss: 1.3765\n",
      "Epoch 64/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 138ms/step - accuracy: 0.9906 - loss: 0.0282 - val_accuracy: 0.7921 - val_loss: 1.3065\n",
      "Epoch 65/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9909 - loss: 0.0272 - val_accuracy: 0.7880 - val_loss: 1.4712\n",
      "Epoch 66/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9900 - loss: 0.0302 - val_accuracy: 0.7906 - val_loss: 1.4606\n",
      "Epoch 67/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9902 - loss: 0.0319 - val_accuracy: 0.7927 - val_loss: 1.4451\n",
      "Epoch 68/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9907 - loss: 0.0283 - val_accuracy: 0.7930 - val_loss: 1.5061\n",
      "Epoch 69/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9901 - loss: 0.0320 - val_accuracy: 0.7995 - val_loss: 1.4807\n",
      "Epoch 70/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 145ms/step - accuracy: 0.9922 - loss: 0.0250 - val_accuracy: 0.7882 - val_loss: 1.4912\n",
      "Epoch 71/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9897 - loss: 0.0309 - val_accuracy: 0.7899 - val_loss: 1.4587\n",
      "Epoch 72/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 143ms/step - accuracy: 0.9902 - loss: 0.0300 - val_accuracy: 0.7971 - val_loss: 1.4125\n",
      "Epoch 73/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 143ms/step - accuracy: 0.9909 - loss: 0.0285 - val_accuracy: 0.7956 - val_loss: 1.5479\n",
      "Epoch 74/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9896 - loss: 0.0321 - val_accuracy: 0.7959 - val_loss: 1.4559\n",
      "Epoch 75/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9899 - loss: 0.0308 - val_accuracy: 0.7865 - val_loss: 1.5682\n",
      "Epoch 76/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9921 - loss: 0.0229 - val_accuracy: 0.8006 - val_loss: 1.4289\n",
      "Epoch 77/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9921 - loss: 0.0252 - val_accuracy: 0.7954 - val_loss: 1.5407\n",
      "Epoch 78/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0268 - val_accuracy: 0.7924 - val_loss: 1.5964\n",
      "Epoch 79/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9907 - loss: 0.0288 - val_accuracy: 0.7945 - val_loss: 1.4972\n",
      "Epoch 80/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0262 - val_accuracy: 0.7842 - val_loss: 1.7286\n",
      "Epoch 81/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9904 - loss: 0.0289 - val_accuracy: 0.7914 - val_loss: 1.5124\n",
      "Epoch 82/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 141ms/step - accuracy: 0.9911 - loss: 0.0267 - val_accuracy: 0.7921 - val_loss: 1.5637\n",
      "Epoch 83/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9919 - loss: 0.0248 - val_accuracy: 0.8007 - val_loss: 1.4739\n",
      "Epoch 84/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9922 - loss: 0.0230 - val_accuracy: 0.7912 - val_loss: 1.5608\n",
      "Epoch 85/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0241 - val_accuracy: 0.8009 - val_loss: 1.6298\n",
      "Epoch 86/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 141ms/step - accuracy: 0.9906 - loss: 0.0277 - val_accuracy: 0.7989 - val_loss: 1.5836\n",
      "Epoch 87/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9925 - loss: 0.0259 - val_accuracy: 0.7984 - val_loss: 1.5211\n",
      "Epoch 88/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9927 - loss: 0.0207 - val_accuracy: 0.7985 - val_loss: 1.6635\n",
      "Epoch 89/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9908 - loss: 0.0274 - val_accuracy: 0.8016 - val_loss: 1.5226\n",
      "Epoch 90/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9933 - loss: 0.0201 - val_accuracy: 0.7969 - val_loss: 1.5441\n",
      "Epoch 91/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0267 - val_accuracy: 0.7961 - val_loss: 1.5785\n",
      "Epoch 92/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9905 - loss: 0.0294 - val_accuracy: 0.7909 - val_loss: 1.6028\n",
      "Epoch 93/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9921 - loss: 0.0234 - val_accuracy: 0.7957 - val_loss: 1.6007\n",
      "Epoch 94/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9920 - loss: 0.0243 - val_accuracy: 0.7990 - val_loss: 1.6264\n",
      "Epoch 95/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9926 - loss: 0.0242 - val_accuracy: 0.7996 - val_loss: 1.6772\n",
      "Epoch 96/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9923 - loss: 0.0223 - val_accuracy: 0.7944 - val_loss: 1.5647\n",
      "Epoch 97/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9934 - loss: 0.0207 - val_accuracy: 0.7991 - val_loss: 1.5542\n",
      "Epoch 98/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 141ms/step - accuracy: 0.9917 - loss: 0.0237 - val_accuracy: 0.8041 - val_loss: 1.6537\n",
      "Epoch 99/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9934 - loss: 0.0208 - val_accuracy: 0.8009 - val_loss: 1.6646\n",
      "Epoch 100/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9933 - loss: 0.0201 - val_accuracy: 0.7975 - val_loss: 1.6526\n",
      "Epoch 101/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9918 - loss: 0.0249 - val_accuracy: 0.7824 - val_loss: 1.8266\n",
      "Epoch 102/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9916 - loss: 0.0268 - val_accuracy: 0.7959 - val_loss: 1.6479\n",
      "Epoch 103/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 140ms/step - accuracy: 0.9922 - loss: 0.0227 - val_accuracy: 0.7912 - val_loss: 1.7565\n",
      "Epoch 104/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 140ms/step - accuracy: 0.9921 - loss: 0.0222 - val_accuracy: 0.8045 - val_loss: 1.5786\n",
      "Epoch 105/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9941 - loss: 0.0171 - val_accuracy: 0.7974 - val_loss: 1.6057\n",
      "Epoch 106/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 140ms/step - accuracy: 0.9913 - loss: 0.0278 - val_accuracy: 0.7992 - val_loss: 1.5721\n",
      "Epoch 107/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9939 - loss: 0.0186 - val_accuracy: 0.7990 - val_loss: 1.7081\n",
      "Epoch 108/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9926 - loss: 0.0228 - val_accuracy: 0.7994 - val_loss: 1.6305\n",
      "Epoch 109/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9932 - loss: 0.0212 - val_accuracy: 0.7904 - val_loss: 1.6908\n",
      "Epoch 110/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 139ms/step - accuracy: 0.9922 - loss: 0.0222 - val_accuracy: 0.8012 - val_loss: 1.6170\n",
      "Epoch 111/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9945 - loss: 0.0159 - val_accuracy: 0.7981 - val_loss: 1.7884\n",
      "Epoch 112/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9919 - loss: 0.0256 - val_accuracy: 0.7893 - val_loss: 1.7079\n",
      "Epoch 113/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 138ms/step - accuracy: 0.9922 - loss: 0.0231 - val_accuracy: 0.7972 - val_loss: 1.7352\n",
      "Epoch 114/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 138ms/step - accuracy: 0.9939 - loss: 0.0190 - val_accuracy: 0.7999 - val_loss: 1.6554\n",
      "Epoch 115/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9921 - loss: 0.0232 - val_accuracy: 0.7975 - val_loss: 1.8069\n"
     ]
    }
   ],
   "source": [
    "history = model_cnn.fit(X_train, y_train,\n",
    "                    batch_size=70,  #Se han probado diferentes, este parece que tiene buena relacion tiempo/calidad\n",
    "                    epochs=115,  \n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val),  \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 horas y 8 minutos de entrenamiento.  \n",
    ">Obviamente dependerá del ordenador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EVALUACION CONTRA EL TEST** \n",
    ">**El modelo, en su entrenamiento, no ha visto estos ni de refilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss en el conjunto de prueba:: 1.5009\n",
      "Accuracy en el conjunto de prueba:: 81.17%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Loss en el conjunto de prueba:: {loss:.4f}')\n",
    "print(f'Accuracy en el conjunto de prueba:: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss en el conjunto de prueba: 1.5009  \n",
    "Accuracy en el conjunto de prueba:: 81.17%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **En un principio, una precisión del 81.17% puede parecer relativamente baja, pero es un resultado aceptable para el reconocimiento de expresiones faciales, especialmente considerando el conjunto de datos FER2013 utilizado. El Ministerio de Bienestar Emocional debería estar satisfecho con este desempeño inicial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREDICCIONES CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_cnn.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CLASSIFICATION REPORT CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Enfado       0.80      0.73      0.76      1797\n",
      "        Asco       0.98      0.99      0.98      1798\n",
      "       Miedo       0.71      0.71      0.71      1798\n",
      "   Felicidad       0.86      0.88      0.87      1798\n",
      "    Tristeza       0.70      0.71      0.71      1798\n",
      "    Sorpresa       0.90      0.91      0.90      1798\n",
      "     Neutral       0.73      0.76      0.74      1798\n",
      "\n",
      "    accuracy                           0.81     12585\n",
      "   macro avg       0.81      0.81      0.81     12585\n",
      "weighted avg       0.81      0.81      0.81     12585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Enfado', 'Asco', 'Miedo', 'Felicidad', 'Tristeza', 'Sorpresa', 'Neutral']\n",
    "print(classification_report(y_test, y_pred_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GUARDADO DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_dir = 'modelos_entrenados'\n",
    "\n",
    "model_cnn.save(os.path.join(save_dir, 'modelo_cnn.h5'))\n",
    "\n",
    "save_model(model_cnn, os.path.join(save_dir, 'modelo_cnn.keras'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **El modelo entrenado ha sido guardado en dos formatos diferentes. Aunque ambos, el formato h5 y el formato keras, mantienen la misma calidad de los datos, la elección del formato puede afectar su usabilidad a largo plazo. Por ello, hemos optado por ofrecer ambas opciones al cliente para que pueda seleccionar la más conveniente según sus necesidades futuras.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESUMEN/EXPLICACIÓN DEL MODELO CNN**\n",
    "\n",
    "Este modelo CNN (Convolutional Neural Network) está diseñado para reconocer las expresiones faciales en fotografias en blanco y negro de 48x48 píxeles, segun \n",
    "chat gtp son algo así como un cerebro digital que aprende a identificar emociones (realmente expresiones faciales) en fotos. 🧠\n",
    "\n",
    "- **Capas Convolucionales**: Son como filtros que detectan características como bordes y texturas en las imágenes  \n",
    "propias de cada expresion, es decir busca las caracteriasticas comunes a cada una de las 7 expresiones faciales \n",
    "que este modelo reconoce, otorgandole una etiqueta a cada una.    \n",
    "\n",
    "- **Capas Densas**: Son como capas finales que interpretan estas características para decidir qué emoción se ve en la imagen.    \n",
    "Digamos que es como un inspector de policia que ve las pruebas (las caracteristicas) y dice \"El sujeto está feliz\"  \n",
    "  \n",
    "- **Dropout**: Es una técnica para evitar que el modelo memorice demasiado y pueda generalizar mejor.    \n",
    "¿Conoces esa fotografia de un hombre durmiendo en un colchón que tiene su forma? pues con dropout se busca evitar eso.  \n",
    "  \n",
    "- **Función de Pérdida**: Durante el entrenamiento (usando la parte de datos reservada para validación) comprueba como   \n",
    "de grande son los fallos que está teniendo.    \n",
    "\n",
    "- **Compilación**:  Es como organizar la granja por la mañana. Preparamos al modelo diciéndole qué errores evitar,    \n",
    "cómo ajustar los pesos de las neuronas (con Adam, ahora lo vemos), y qué métricas usar para ver como de bien está aprendiendo.  \n",
    "  \n",
    "- **Optimizador (Adam)**:  El optimizador Adam ajusta cómo la CNN aprende de los datos. Es como un \"entrenador personal\" (O el jefe de la granja)    \n",
    "que guía al modelo sobre cómo mejorar durante el entrenamiento, ajustando los pesos y sesgos de las neuronas para que pueda    \n",
    "predecir mejor las expresiones faciales en las imágenes.    \n",
    "  \n",
    "- **Callback (EarlyStopping)**: Es como \"el capataz\" durante el entrenamiento. Monitorea como la precisión en los datos de validación    \n",
    "va mejorando y detiene el entrenamiento cuando la precisión deja de mejorar, restaurando los mejores pesos del modelo.    \n",
    "Esto ayuda a prevenir el sobreajuste y asegura que el modelo generalize bien a nuevos datos.    \n",
    "\n",
    "- **Métricas**: Digamos que son como las notas del cole, debemos saber como de bueno es el modelo, para ello  \n",
    "dejamos reservada parte, el test, que no veria en el entrenamiento, para poder ver como predice.  \n",
    "En este caso tenemos dos de las asignaturas mas importantes, **Acuracy** (punteria), o sea... del total de prediciones cuantas  \n",
    "eran buenas, poco mas que decir y **loss**, que es como decir \"como de malas son las prediciones que son incorrectas\" dicho de otro modo, En un modelo que diferenciara aniamales, si se predice \"Gorila\" y la foto era un chimpancé la prediccion, sin ser correcta, no es lo peor del mundo, en cambio si predigera \"berberecho\" estaría muy mal.\n",
    "\n",
    "Entrenamos el modelo con los datos de entrenamiento y luego lo evaluamos para ver si puede predecir emociones en datos que no ha visto.  \n",
    "Segun chatgtp (y cito textualmente) \"¡Es como enseñar a una computadora a leer expresiones faciales! 😊\"\n",
    "\n",
    "\n",
    "Es importante tener en cuenta que la salida del modelo no son strings, no nos dice directamente la emoción, nos da un numero entero, \n",
    "por lo que usar un diccionario se hace neecsario, al menos conocerlos: \n",
    "  \n",
    "- 0 para 'Enfado' 😡\n",
    "- 1 para 'Asco' 🤢\n",
    "- 2 para 'Miedo' 😨\n",
    "- 3 para 'Felicidad' 😄\n",
    "- 4 para 'Tristeza' 😢\n",
    "- 5 para 'Sorpresa' 😮\n",
    "- 6 para 'Neutral' 😐\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imagenes/VGG16.jpg\" alt=\"Imagen creada con inteligencia artificial y editada con Microsoft Paint\" style=\"border-radius: 15px; width: 95%;\">\n",
    "\n",
    "\n",
    "\n",
    "*Imagen creada con inteligencia artificial*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEFINICIÓN DEL MODELO BASE**  \n",
    ">**aquí se define el modelo preentrenado VGG-16, utilizando pesos preentrenados de 'imagenet'**   \n",
    "**y configurando la entrada para imágenes de 48x48 píxeles con 3 canales de color.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AÑADIDO DE CAPAS**  \n",
    ">**Con estas líneas adaptamos el modelo preentrenado VGG16 para que pueda hacer clasificaciones de**    \n",
    "**expresiones faciales en nuestro caso, teniendo como entrada imágenes de 48x48 en escala de grises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pepea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_vgg = models.Sequential()\n",
    "model_vgg.add(layers.Conv2D(3, (1, 1), input_shape=(48, 48, 1)))  # Convertimos a 3 canales (o sea, colores... guiño guiño)\n",
    "model_vgg.add(base_model)\n",
    "model_vgg.add(layers.Flatten())\n",
    "model_vgg.add(layers.Dense(4096, activation='relu'))\n",
    "model_vgg.add(layers.Dense(4096, activation='relu'))\n",
    "model_vgg.add(layers.Dense(7, activation='softmax'))  # 7 expresiones faciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONGELADO DE CAPAS**  \n",
    ">**Congelar las capas del modelo base (VGG-16) significa mantener sus pesos preentrenados,**    \n",
    "**lo que aprovecha el conocimiento aprendido en imágenes generales. Esto acelera el entrenamient**  \n",
    "**y reduce el riesgo de sobreajuste al adaptar el modelo para clasificar expresiones faciales en**    \n",
    "**imágenes de 48x48 píxeles en escala de grises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COMPILADO DEL MODELO VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.compile(optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ENTRENAMIENTO DEL MODELO VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1298s\u001b[0m 836ms/step - accuracy: 0.3686 - loss: 1.6483 - val_accuracy: 0.4880 - val_loss: 1.3291\n",
      "Epoch 2/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 835ms/step - accuracy: 0.5264 - loss: 1.2363 - val_accuracy: 0.5594 - val_loss: 1.1594\n",
      "Epoch 3/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.6132 - loss: 1.0189 - val_accuracy: 0.6048 - val_loss: 1.0534\n",
      "Epoch 4/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.7059 - loss: 0.7962 - val_accuracy: 0.6714 - val_loss: 0.8967\n",
      "Epoch 5/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 836ms/step - accuracy: 0.7921 - loss: 0.5774 - val_accuracy: 0.7242 - val_loss: 0.7895\n",
      "Epoch 6/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.8613 - loss: 0.3987 - val_accuracy: 0.7492 - val_loss: 0.7395\n",
      "Epoch 7/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 834ms/step - accuracy: 0.9116 - loss: 0.2671 - val_accuracy: 0.7778 - val_loss: 0.6931\n",
      "Epoch 8/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 835ms/step - accuracy: 0.9443 - loss: 0.1828 - val_accuracy: 0.8027 - val_loss: 0.6573\n",
      "Epoch 9/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.9601 - loss: 0.1332 - val_accuracy: 0.7960 - val_loss: 0.7144\n",
      "Epoch 10/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 834ms/step - accuracy: 0.9679 - loss: 0.1105 - val_accuracy: 0.8110 - val_loss: 0.6763\n",
      "Epoch 11/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1292s\u001b[0m 834ms/step - accuracy: 0.9724 - loss: 0.0927 - val_accuracy: 0.8087 - val_loss: 0.7282\n",
      "Epoch 12/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1297s\u001b[0m 837ms/step - accuracy: 0.9744 - loss: 0.0859 - val_accuracy: 0.8203 - val_loss: 0.6823\n",
      "Epoch 13/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1441s\u001b[0m 930ms/step - accuracy: 0.9786 - loss: 0.0732 - val_accuracy: 0.8261 - val_loss: 0.6919\n",
      "Epoch 14/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1222s\u001b[0m 789ms/step - accuracy: 0.9790 - loss: 0.0711 - val_accuracy: 0.8243 - val_loss: 0.7208\n",
      "Epoch 15/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1111s\u001b[0m 717ms/step - accuracy: 0.9816 - loss: 0.0616 - val_accuracy: 0.8326 - val_loss: 0.6980\n",
      "Epoch 16/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1093s\u001b[0m 706ms/step - accuracy: 0.9822 - loss: 0.0576 - val_accuracy: 0.8312 - val_loss: 0.7135\n",
      "Epoch 17/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1111s\u001b[0m 717ms/step - accuracy: 0.9843 - loss: 0.0522 - val_accuracy: 0.8264 - val_loss: 0.7793\n",
      "Epoch 18/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1148s\u001b[0m 741ms/step - accuracy: 0.9830 - loss: 0.0566 - val_accuracy: 0.8322 - val_loss: 0.7674\n",
      "Epoch 19/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1163s\u001b[0m 751ms/step - accuracy: 0.9832 - loss: 0.0551 - val_accuracy: 0.8434 - val_loss: 0.6862\n",
      "Epoch 20/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1137s\u001b[0m 734ms/step - accuracy: 0.9845 - loss: 0.0498 - val_accuracy: 0.8439 - val_loss: 0.7552\n",
      "Epoch 21/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1137s\u001b[0m 734ms/step - accuracy: 0.9840 - loss: 0.0520 - val_accuracy: 0.8261 - val_loss: 0.8703\n",
      "Epoch 22/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1136s\u001b[0m 734ms/step - accuracy: 0.9848 - loss: 0.0491 - val_accuracy: 0.8373 - val_loss: 0.7880\n",
      "Epoch 23/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1119s\u001b[0m 723ms/step - accuracy: 0.9846 - loss: 0.0498 - val_accuracy: 0.8451 - val_loss: 0.7631\n",
      "Epoch 24/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1123s\u001b[0m 725ms/step - accuracy: 0.9842 - loss: 0.0516 - val_accuracy: 0.8485 - val_loss: 0.7386\n",
      "Epoch 25/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1123s\u001b[0m 725ms/step - accuracy: 0.9883 - loss: 0.0385 - val_accuracy: 0.8393 - val_loss: 0.8001\n"
     ]
    }
   ],
   "source": [
    "history = model_vgg.fit(X_train, y_train, epochs=25, batch_size=65, \n",
    "                        validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 HORAS Y 30 MINUTOS DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EVALUACION CONTRA EL TEST** \n",
    ">**El modelo, en su entrenamiento, no ha visto estos ni de refilon (si, otra vez)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss en el conjunto de prueba: 0.7857\n",
      "Accuracy en el conjunto de prueba: 84.41%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_vgg.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Loss en el conjunto de prueba: {loss:.4f}')\n",
    "print(f'Accuracy en el conjunto de prueba: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss en el conjunto de prueba: 0.7857  \n",
    "Accuracy en el conjunto de prueba: 84.41%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREDICCIONES VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_vgg = model_cnn.predict(X_test)\n",
    "y_pred_classes_vgg = np.argmax(y_pred_vgg, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CLASSIFICATION REPORT CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Enfado       0.80      0.73      0.76      1797\n",
      "        Asco       0.98      0.99      0.98      1798\n",
      "       Miedo       0.71      0.71      0.71      1798\n",
      "   Felicidad       0.86      0.88      0.87      1798\n",
      "    Tristeza       0.70      0.71      0.71      1798\n",
      "    Sorpresa       0.90      0.91      0.90      1798\n",
      "     Neutral       0.73      0.76      0.74      1798\n",
      "\n",
      "    accuracy                           0.81     12585\n",
      "   macro avg       0.81      0.81      0.81     12585\n",
      "weighted avg       0.81      0.81      0.81     12585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Enfado', 'Asco', 'Miedo', 'Felicidad', 'Tristeza', 'Sorpresa', 'Neutral']\n",
    "print(classification_report(y_test, y_pred_classes_vgg, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GUARDADO DEL MODELO VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_vgg.save(os.path.join(save_dir, 'modelo_vgg.h5'))\n",
    "\n",
    "save_model(model_vgg, os.path.join(save_dir, 'modelo_vgg.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESUMEN/EXPLICACIÓN DEL MODELO VGG16**\n",
    "\n",
    "Este modelo VGG16 modificado está diseñado para reconocer expresiones faciales en fotografías en blanco y negro de 48x48 píxeles, aunque es necesario convertirlas a 3 canales (RGB) para poder utilizar el modelo base VGG16 preentrenado en ImageNet.\n",
    "\n",
    "> 🤫 **Nota**: \"RGB\" significa \"Red, Green, Blue\" en inglés, que son los colores primarios usados en pantallas para mostrar colores.\n",
    "\n",
    "### **Capas Convolucionales (VGG16):**\n",
    "\n",
    "Utilizaremos el **modelo VGG16** como base, el cual ha sido previamente entrenado con un gran conjunto de datos generales.  \n",
    "Aprovecharemos los pesos y conocimientos adquiridos durante su entrenamiento, que consiste en múltiples capas convolucionales  \n",
    "y de agrupación para detectar características complejas como bordes, texturas y patrones en las imágenes.  \n",
    "\n",
    "### **Capas Densas Personalizadas:**\n",
    "\n",
    "Se añaden capas densas para interpretar las características extraídas por VGG16 y clasificar las imágenes en una de las 7 expresiones faciales.    \n",
    "Estas capas densas actúan como \"inspectores\" que determinan qué emoción se observa en la imagen.  \n",
    "\n",
    "### **Congelación de Capas:**\n",
    "\n",
    "Todas las capas de VGG16 se congelan para mantener los pesos preentrenados obtenidos de ImageNet. Esto aprovecha el conocimiento previo      \n",
    "del modelo base y evita que se sobrescriban durante el entrenamiento con nuevos datos.  \n",
    "\n",
    "### **Compilación del Modelo VGG16:**\n",
    "\n",
    "El modelo se compila utilizando el optimizador Adam con una tasa de aprendizaje de 0.0005 y la función de pérdida **'sparse_categorical_crossentropy'**.    \n",
    "Esto prepara al modelo para el entrenamiento, especificando cómo debe actualizarse durante el proceso de aprendizaje.  \n",
    "\n",
    "> **Compilar** en el contexto de la programación y el aprendizaje automático significa preparar un programa o modelo para su ejecución o entrenamiento,  \n",
    "optimizando el código fuente y las operaciones para la máquina.  \n",
    ">\n",
    "> **'sparse_categorical_crossentropy'** es una función de pérdida utilizada en problemas de clasificación donde las etiquetas son números enteros (sparse),  \n",
    "es decir, no están codificadas en formato one-hot. Calcula la discrepancia entre las etiquetas reales y las predicciones del modelo para mejorar su  \n",
    "capacidad de clasificación.\n",
    "\n",
    "### **Entrenamiento del Modelo:**\n",
    "\n",
    "El modelo se entrena durante 25 épocas utilizando datos de entrenamiento con lotes de 65 imágenes cada uno. Se utiliza un conjunto de validación  \n",
    "para monitorear el rendimiento y evitar el sobreajuste.  \n",
    "\n",
    "### **Evaluación del Modelo:**\n",
    "\n",
    "Después del entrenamiento, se evalúa el modelo utilizando un conjunto de prueba reservado previamente. Se calcula la precisión (accuracy) y otras  \n",
    "métricas para comprender qué tan bien generaliza el modelo con nuevos datos.  \n",
    "\n",
    "### **Guardado del Modelo:**\n",
    "\n",
    "Hacemos lo mismo que con el modelo anterior, obviamente cambiando los nombres.  \n",
    "\n",
    "En este caso, la salida del modelo también es numérica, por lo que vuelve a ser necesario un diccionario para interpretar las predicciones. El diccionario es el mismo.\n",
    "- 0 para 'Enfado' 😡\n",
    "- 1 para 'Asco' 🤢\n",
    "- 2 para 'Miedo' 😨\n",
    "- 3 para 'Felicidad' 😄\n",
    "- 4 para 'Tristeza' 😢\n",
    "- 5 para 'Sorpresa' 😮\n",
    "- 6 para 'Neutral' 😐\n",
    "\n",
    "**ChatGPT dice**: ¡El modelo VGG16 es una poderosa herramienta para el reconocimiento de expresiones faciales, adaptada y optimizada para este desafío específico!  \n",
    "\n",
    "## **¿POR QUÉ USAR DOS MODELOS?** \n",
    "\n",
    "Reconocer expresiones faciales es una tarea relativamente simple para los seres humanos, pero para los modelos de aprendizaje automático no es tan fácil. Interpretar y distinguir diferencias sutiles entre expresiones emocionales puede ser complicado para un solo modelo. Por lo tanto, para aumentar la confianza en las predicciones, se han entrenado dos modelos.\n",
    "\n",
    "Ambos modelos están entrenados con el mismo conjunto de datos (Fer2013, acuerdate) y son bastante similares en su arquitectura. Sin embargo, el programa realiza predicciones con ambos modelos y selecciona la predicción que tenga el mayor nivel de confianza. En casos donde ambos modelos predicen diferentes emociones pero con igual nivel de confianza, se requerirá una revisión humana para tomar la decisión final.\n",
    "\n",
    "Para mitigar el sesgo y sobreajuste debido a la utilización del mismo conjunto de datos, ambos modelos serán reentrenados con un nuevo dataset. Esto permitirá que los modelos generalicen mejor las predicciones a nuevas imágenes y situaciones.\n",
    "\n",
    "Este enfoque de utilizar dos modelos y validar las predicciones con criterios de confianza y revisión humana ayuda a mejorar la precisión y fiabilidad del sistema de reconocimiento de expresiones faciales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
