{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imagenes/bannermodelado_cnn.jpg\" alt=\"Imagen creada con inteligencia artificial y editada con Microsoft Paint\" style=\"border-radius: 15px; width: 95%;\">\n",
    "\n",
    "*Imagen creada con inteligencia artificial*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BIBLIOTECAS USADAS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report \n",
    "import os\n",
    "from keras.models import save_model \n",
    "from tensorflow.keras.applications import VGG16  \n",
    "from tensorflow.keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CARGA DEL CONJUNTO DE DATOS 'FER-2013'** \n",
    ">**Realmente no cargamos el dataset 'fer2013' tal cual; cargamos un dataset obtenido tras aplicar t√©cnicas de data augmentation. En el Jupyter Notebook 'Data_augmentation_fer2013' se realiza y explica el proceso.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset ha sido cargado correctamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datos/fer2013/fer2013_blc_todos_rotados.csv')\n",
    "print(\"El dataset ha sido cargado correctamente.\") \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREPARACI√ìN PREVIA AL SPLIT**  \n",
    ">>**La columna 'Usage' no nos es √∫til. Le ponemos cara de asco y la borramos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Usage'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SEPARAMOS \"X\" E \"y\" PIXELES Y ETIQUETAS.** \n",
    "> **La columna 'emotion' contiene las etiquetas, y la columna 'pixels' contiene las cadenas de n√∫meros que dan lugar a las fotograf√≠as.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['emotion'])  \n",
    "y = df['emotion']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONVERSI√ìN A ARRAY Y NORMALIZACI√ìN DE LA COLUMNA 'PIXELS'**  \n",
    ">**Los modelos, al menos con los que trabajaremos, necesitan arrays de NumPy; no pueden trabajar con cadenas. Adem√°s, deben estar normalizados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del array X_train_pixels: (125846, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "def string_to_image_array(string):\n",
    "    pixels = np.array(string.split(), dtype=np.float32)\n",
    "    return pixels.reshape((48, 48, 1))\n",
    "\n",
    "X_pixels = np.array([string_to_image_array(pixels) for pixels in X['pixels']])\n",
    "X_pixels = X_pixels.astype('float32')\n",
    "X_pixels /= 255.0 \n",
    "\n",
    "print(\"Shape del array X_train_pixels:\", X_pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SPLIT**  \n",
    ">**Dividiremos nuestro dataset en tres partes: train, validaci√≥n y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øSe han separado de forma balanceada?\n",
      "Tanto por 1 de clases en y_train:\n",
      "emotion\n",
      "1    0.142864\n",
      "0    0.142864\n",
      "3    0.142854\n",
      "6    0.142854\n",
      "4    0.142854\n",
      "5    0.142854\n",
      "2    0.142854\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tanto por 1 de clases en y_val:\n",
      "emotion\n",
      "2    0.142868\n",
      "6    0.142868\n",
      "5    0.142868\n",
      "3    0.142868\n",
      "0    0.142868\n",
      "4    0.142868\n",
      "1    0.142789\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tanto por 1 de clases en y_test:\n",
      "emotion\n",
      "1    0.142868\n",
      "2    0.142868\n",
      "3    0.142868\n",
      "4    0.142868\n",
      "5    0.142868\n",
      "6    0.142868\n",
      "0    0.142789\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_pixels, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "print(\"¬øSe han separado de forma balanceada?\")\n",
    "print(\"Tanto por 1 de clases en y_train:\")\n",
    "print(y_train.value_counts(True))\n",
    "\n",
    "print(\"\\nTanto por 1 de clases en y_val:\")\n",
    "print(y_val.value_counts(True))\n",
    "\n",
    "print(\"\\nTanto por 1 de clases en y_test:\")\n",
    "print(y_test.value_counts(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">S√≠, la estratificaci√≥n ha sido correcta. El porcentaje de cada clase es muy similar en train, validaci√≥n y test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODELOS:**  \n",
    ">**En primer lugar, definiremos, entrenaremos y evaluaremos un modelo de Red Neuronal Convolucional (CNN). En segundo lugar, haremos lo mismo con un modelo de Transfer Learning basado en VGG16.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEFINICI√ìN DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pepea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "\n",
    "# Primera capa convolucional\n",
    "model_cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1))) #El modelo trabaja con arrays de 48x48 de una sola dimension, o sea capas... En blanco y negro, hablando en plata.\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Segunda capa convolucional\n",
    "model_cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Tercera capa convolucional\n",
    "model_cnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Cuarta capa convolucional\n",
    "model_cnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capa de flatten para conectar con la capa densa\n",
    "model_cnn.add(Flatten())\n",
    "\n",
    "# Quinta capa densa (totalmente conectada)\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))  # Dropout para reducir overfitting\n",
    "\n",
    "# Sexta capa densa (totalmente conectada)\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))  # Dropout para reducir overfitting\n",
    "\n",
    "# Capa de salida (no se cambia)\n",
    "model_cnn.add(Dense(7, activation='softmax'))  # Capa de salida con 7 clases (expresiones faciales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COMPILADO DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(loss='sparse_categorical_crossentropy',  \n",
    "              optimizer=Adam(learning_rate=0.0002),  #He ido probando, este ha dado muy buen resultado\n",
    "              metrics=['accuracy'])  # Buscamos la mayor presicion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CALLBACK PARA EL MODELO CNN**   \n",
    ">**Para detener el entrenamiento si la mejora en la m√©trica se detiene.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=40, restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**El hiperpar√°metro 'patience' se llama as√≠ por algo, es decir... si aumentas 'patience', el modelo tendr√° m√°s paciencia, seguir√° durante m√°s √©pocas sin ver mejoras (a la espera de que mejore en otra o se acaben las √©pocas).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ENTRENAMIENTO DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 132ms/step - accuracy: 0.2086 - loss: 1.8715 - val_accuracy: 0.4545 - val_loss: 1.4219\n",
      "Epoch 2/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 135ms/step - accuracy: 0.4758 - loss: 1.3990 - val_accuracy: 0.5593 - val_loss: 1.1799\n",
      "Epoch 3/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 130ms/step - accuracy: 0.5653 - loss: 1.1676 - val_accuracy: 0.6124 - val_loss: 1.0382\n",
      "Epoch 4/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.6221 - loss: 1.0113 - val_accuracy: 0.6411 - val_loss: 0.9413\n",
      "Epoch 5/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 126ms/step - accuracy: 0.6646 - loss: 0.9029 - val_accuracy: 0.6610 - val_loss: 0.8988\n",
      "Epoch 6/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 126ms/step - accuracy: 0.6956 - loss: 0.8176 - val_accuracy: 0.6718 - val_loss: 0.8646\n",
      "Epoch 7/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 127ms/step - accuracy: 0.7317 - loss: 0.7245 - val_accuracy: 0.6888 - val_loss: 0.8246\n",
      "Epoch 8/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.7647 - loss: 0.6446 - val_accuracy: 0.7052 - val_loss: 0.8139\n",
      "Epoch 9/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.7982 - loss: 0.5612 - val_accuracy: 0.7046 - val_loss: 0.8346\n",
      "Epoch 10/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.8298 - loss: 0.4788 - val_accuracy: 0.7275 - val_loss: 0.7691\n",
      "Epoch 11/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.8592 - loss: 0.4026 - val_accuracy: 0.7311 - val_loss: 0.7939\n",
      "Epoch 12/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 123ms/step - accuracy: 0.8796 - loss: 0.3459 - val_accuracy: 0.7443 - val_loss: 0.8088\n",
      "Epoch 13/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 123ms/step - accuracy: 0.8993 - loss: 0.2938 - val_accuracy: 0.7499 - val_loss: 0.8141\n",
      "Epoch 14/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 123ms/step - accuracy: 0.9145 - loss: 0.2462 - val_accuracy: 0.7608 - val_loss: 0.8142\n",
      "Epoch 15/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 123ms/step - accuracy: 0.9283 - loss: 0.2138 - val_accuracy: 0.7599 - val_loss: 0.8552\n",
      "Epoch 16/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9390 - loss: 0.1831 - val_accuracy: 0.7650 - val_loss: 0.9153\n",
      "Epoch 17/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9466 - loss: 0.1618 - val_accuracy: 0.7627 - val_loss: 0.9173\n",
      "Epoch 18/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9511 - loss: 0.1477 - val_accuracy: 0.7692 - val_loss: 0.9383\n",
      "Epoch 19/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9576 - loss: 0.1285 - val_accuracy: 0.7683 - val_loss: 0.9643\n",
      "Epoch 20/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9610 - loss: 0.1166 - val_accuracy: 0.7770 - val_loss: 0.9681\n",
      "Epoch 21/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.9652 - loss: 0.1070 - val_accuracy: 0.7650 - val_loss: 1.0165\n",
      "Epoch 22/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.9671 - loss: 0.1058 - val_accuracy: 0.7739 - val_loss: 0.9934\n",
      "Epoch 23/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 128ms/step - accuracy: 0.9689 - loss: 0.0955 - val_accuracy: 0.7708 - val_loss: 0.9857\n",
      "Epoch 24/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 127ms/step - accuracy: 0.9724 - loss: 0.0853 - val_accuracy: 0.7681 - val_loss: 1.0764\n",
      "Epoch 25/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 127ms/step - accuracy: 0.9735 - loss: 0.0811 - val_accuracy: 0.7854 - val_loss: 1.0062\n",
      "Epoch 26/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.9749 - loss: 0.0771 - val_accuracy: 0.7672 - val_loss: 1.1106\n",
      "Epoch 27/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 124ms/step - accuracy: 0.9756 - loss: 0.0755 - val_accuracy: 0.7772 - val_loss: 1.1046\n",
      "Epoch 28/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9778 - loss: 0.0701 - val_accuracy: 0.7777 - val_loss: 1.1000\n",
      "Epoch 29/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 124ms/step - accuracy: 0.9784 - loss: 0.0660 - val_accuracy: 0.7897 - val_loss: 1.0095\n",
      "Epoch 30/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9790 - loss: 0.0670 - val_accuracy: 0.7818 - val_loss: 1.1497\n",
      "Epoch 31/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 124ms/step - accuracy: 0.9799 - loss: 0.0634 - val_accuracy: 0.7776 - val_loss: 1.1684\n",
      "Epoch 32/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9812 - loss: 0.0593 - val_accuracy: 0.7707 - val_loss: 1.1800\n",
      "Epoch 33/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 125ms/step - accuracy: 0.9820 - loss: 0.0584 - val_accuracy: 0.7853 - val_loss: 1.1035\n",
      "Epoch 34/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9826 - loss: 0.0534 - val_accuracy: 0.7852 - val_loss: 1.1058\n",
      "Epoch 35/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.9833 - loss: 0.0497 - val_accuracy: 0.7860 - val_loss: 1.1394\n",
      "Epoch 36/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9818 - loss: 0.0557 - val_accuracy: 0.7836 - val_loss: 1.1673\n",
      "Epoch 37/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9843 - loss: 0.0506 - val_accuracy: 0.7919 - val_loss: 1.1433\n",
      "Epoch 38/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 125ms/step - accuracy: 0.9839 - loss: 0.0479 - val_accuracy: 0.7851 - val_loss: 1.1828\n",
      "Epoch 39/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9845 - loss: 0.0491 - val_accuracy: 0.7885 - val_loss: 1.2766\n",
      "Epoch 40/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9856 - loss: 0.0432 - val_accuracy: 0.7865 - val_loss: 1.2260\n",
      "Epoch 41/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9846 - loss: 0.0469 - val_accuracy: 0.7874 - val_loss: 1.2814\n",
      "Epoch 42/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9851 - loss: 0.0459 - val_accuracy: 0.7860 - val_loss: 1.3282\n",
      "Epoch 43/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9854 - loss: 0.0450 - val_accuracy: 0.7877 - val_loss: 1.2442\n",
      "Epoch 44/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 125ms/step - accuracy: 0.9875 - loss: 0.0388 - val_accuracy: 0.7851 - val_loss: 1.3001\n",
      "Epoch 45/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9853 - loss: 0.0440 - val_accuracy: 0.7814 - val_loss: 1.3145\n",
      "Epoch 46/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 124ms/step - accuracy: 0.9856 - loss: 0.0442 - val_accuracy: 0.7868 - val_loss: 1.3002\n",
      "Epoch 47/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 136ms/step - accuracy: 0.9873 - loss: 0.0387 - val_accuracy: 0.7898 - val_loss: 1.2733\n",
      "Epoch 48/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 164ms/step - accuracy: 0.9869 - loss: 0.0392 - val_accuracy: 0.7943 - val_loss: 1.2633\n",
      "Epoch 49/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 138ms/step - accuracy: 0.9871 - loss: 0.0418 - val_accuracy: 0.7882 - val_loss: 1.2836\n",
      "Epoch 50/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 130ms/step - accuracy: 0.9879 - loss: 0.0367 - val_accuracy: 0.7865 - val_loss: 1.3387\n",
      "Epoch 51/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 129ms/step - accuracy: 0.9876 - loss: 0.0375 - val_accuracy: 0.7851 - val_loss: 1.2547\n",
      "Epoch 52/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 129ms/step - accuracy: 0.9880 - loss: 0.0376 - val_accuracy: 0.7915 - val_loss: 1.3295\n",
      "Epoch 53/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 129ms/step - accuracy: 0.9887 - loss: 0.0342 - val_accuracy: 0.7932 - val_loss: 1.2954\n",
      "Epoch 54/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 130ms/step - accuracy: 0.9891 - loss: 0.0325 - val_accuracy: 0.7886 - val_loss: 1.3363\n",
      "Epoch 55/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 129ms/step - accuracy: 0.9882 - loss: 0.0360 - val_accuracy: 0.7871 - val_loss: 1.3439\n",
      "Epoch 56/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 129ms/step - accuracy: 0.9892 - loss: 0.0347 - val_accuracy: 0.7898 - val_loss: 1.3886\n",
      "Epoch 57/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 130ms/step - accuracy: 0.9895 - loss: 0.0317 - val_accuracy: 0.7896 - val_loss: 1.3515\n",
      "Epoch 58/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 131ms/step - accuracy: 0.9876 - loss: 0.0386 - val_accuracy: 0.7971 - val_loss: 1.3867\n",
      "Epoch 59/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 132ms/step - accuracy: 0.9881 - loss: 0.0375 - val_accuracy: 0.7936 - val_loss: 1.3109\n",
      "Epoch 60/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 134ms/step - accuracy: 0.9907 - loss: 0.0274 - val_accuracy: 0.7945 - val_loss: 1.3478\n",
      "Epoch 61/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 135ms/step - accuracy: 0.9901 - loss: 0.0275 - val_accuracy: 0.7835 - val_loss: 1.3871\n",
      "Epoch 62/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 136ms/step - accuracy: 0.9874 - loss: 0.0415 - val_accuracy: 0.7924 - val_loss: 1.4396\n",
      "Epoch 63/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 137ms/step - accuracy: 0.9901 - loss: 0.0308 - val_accuracy: 0.7961 - val_loss: 1.3765\n",
      "Epoch 64/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 138ms/step - accuracy: 0.9906 - loss: 0.0282 - val_accuracy: 0.7921 - val_loss: 1.3065\n",
      "Epoch 65/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9909 - loss: 0.0272 - val_accuracy: 0.7880 - val_loss: 1.4712\n",
      "Epoch 66/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9900 - loss: 0.0302 - val_accuracy: 0.7906 - val_loss: 1.4606\n",
      "Epoch 67/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9902 - loss: 0.0319 - val_accuracy: 0.7927 - val_loss: 1.4451\n",
      "Epoch 68/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9907 - loss: 0.0283 - val_accuracy: 0.7930 - val_loss: 1.5061\n",
      "Epoch 69/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9901 - loss: 0.0320 - val_accuracy: 0.7995 - val_loss: 1.4807\n",
      "Epoch 70/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 145ms/step - accuracy: 0.9922 - loss: 0.0250 - val_accuracy: 0.7882 - val_loss: 1.4912\n",
      "Epoch 71/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9897 - loss: 0.0309 - val_accuracy: 0.7899 - val_loss: 1.4587\n",
      "Epoch 72/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 143ms/step - accuracy: 0.9902 - loss: 0.0300 - val_accuracy: 0.7971 - val_loss: 1.4125\n",
      "Epoch 73/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 143ms/step - accuracy: 0.9909 - loss: 0.0285 - val_accuracy: 0.7956 - val_loss: 1.5479\n",
      "Epoch 74/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 142ms/step - accuracy: 0.9896 - loss: 0.0321 - val_accuracy: 0.7959 - val_loss: 1.4559\n",
      "Epoch 75/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9899 - loss: 0.0308 - val_accuracy: 0.7865 - val_loss: 1.5682\n",
      "Epoch 76/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9921 - loss: 0.0229 - val_accuracy: 0.8006 - val_loss: 1.4289\n",
      "Epoch 77/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9921 - loss: 0.0252 - val_accuracy: 0.7954 - val_loss: 1.5407\n",
      "Epoch 78/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0268 - val_accuracy: 0.7924 - val_loss: 1.5964\n",
      "Epoch 79/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9907 - loss: 0.0288 - val_accuracy: 0.7945 - val_loss: 1.4972\n",
      "Epoch 80/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0262 - val_accuracy: 0.7842 - val_loss: 1.7286\n",
      "Epoch 81/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9904 - loss: 0.0289 - val_accuracy: 0.7914 - val_loss: 1.5124\n",
      "Epoch 82/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 141ms/step - accuracy: 0.9911 - loss: 0.0267 - val_accuracy: 0.7921 - val_loss: 1.5637\n",
      "Epoch 83/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9919 - loss: 0.0248 - val_accuracy: 0.8007 - val_loss: 1.4739\n",
      "Epoch 84/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9922 - loss: 0.0230 - val_accuracy: 0.7912 - val_loss: 1.5608\n",
      "Epoch 85/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0241 - val_accuracy: 0.8009 - val_loss: 1.6298\n",
      "Epoch 86/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 141ms/step - accuracy: 0.9906 - loss: 0.0277 - val_accuracy: 0.7989 - val_loss: 1.5836\n",
      "Epoch 87/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9925 - loss: 0.0259 - val_accuracy: 0.7984 - val_loss: 1.5211\n",
      "Epoch 88/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9927 - loss: 0.0207 - val_accuracy: 0.7985 - val_loss: 1.6635\n",
      "Epoch 89/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9908 - loss: 0.0274 - val_accuracy: 0.8016 - val_loss: 1.5226\n",
      "Epoch 90/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 142ms/step - accuracy: 0.9933 - loss: 0.0201 - val_accuracy: 0.7969 - val_loss: 1.5441\n",
      "Epoch 91/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9914 - loss: 0.0267 - val_accuracy: 0.7961 - val_loss: 1.5785\n",
      "Epoch 92/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9905 - loss: 0.0294 - val_accuracy: 0.7909 - val_loss: 1.6028\n",
      "Epoch 93/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9921 - loss: 0.0234 - val_accuracy: 0.7957 - val_loss: 1.6007\n",
      "Epoch 94/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9920 - loss: 0.0243 - val_accuracy: 0.7990 - val_loss: 1.6264\n",
      "Epoch 95/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9926 - loss: 0.0242 - val_accuracy: 0.7996 - val_loss: 1.6772\n",
      "Epoch 96/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9923 - loss: 0.0223 - val_accuracy: 0.7944 - val_loss: 1.5647\n",
      "Epoch 97/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9934 - loss: 0.0207 - val_accuracy: 0.7991 - val_loss: 1.5542\n",
      "Epoch 98/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 141ms/step - accuracy: 0.9917 - loss: 0.0237 - val_accuracy: 0.8041 - val_loss: 1.6537\n",
      "Epoch 99/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9934 - loss: 0.0208 - val_accuracy: 0.8009 - val_loss: 1.6646\n",
      "Epoch 100/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 141ms/step - accuracy: 0.9933 - loss: 0.0201 - val_accuracy: 0.7975 - val_loss: 1.6526\n",
      "Epoch 101/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9918 - loss: 0.0249 - val_accuracy: 0.7824 - val_loss: 1.8266\n",
      "Epoch 102/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 140ms/step - accuracy: 0.9916 - loss: 0.0268 - val_accuracy: 0.7959 - val_loss: 1.6479\n",
      "Epoch 103/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 140ms/step - accuracy: 0.9922 - loss: 0.0227 - val_accuracy: 0.7912 - val_loss: 1.7565\n",
      "Epoch 104/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 140ms/step - accuracy: 0.9921 - loss: 0.0222 - val_accuracy: 0.8045 - val_loss: 1.5786\n",
      "Epoch 105/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9941 - loss: 0.0171 - val_accuracy: 0.7974 - val_loss: 1.6057\n",
      "Epoch 106/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 140ms/step - accuracy: 0.9913 - loss: 0.0278 - val_accuracy: 0.7992 - val_loss: 1.5721\n",
      "Epoch 107/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9939 - loss: 0.0186 - val_accuracy: 0.7990 - val_loss: 1.7081\n",
      "Epoch 108/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9926 - loss: 0.0228 - val_accuracy: 0.7994 - val_loss: 1.6305\n",
      "Epoch 109/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9932 - loss: 0.0212 - val_accuracy: 0.7904 - val_loss: 1.6908\n",
      "Epoch 110/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 139ms/step - accuracy: 0.9922 - loss: 0.0222 - val_accuracy: 0.8012 - val_loss: 1.6170\n",
      "Epoch 111/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9945 - loss: 0.0159 - val_accuracy: 0.7981 - val_loss: 1.7884\n",
      "Epoch 112/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9919 - loss: 0.0256 - val_accuracy: 0.7893 - val_loss: 1.7079\n",
      "Epoch 113/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 138ms/step - accuracy: 0.9922 - loss: 0.0231 - val_accuracy: 0.7972 - val_loss: 1.7352\n",
      "Epoch 114/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 138ms/step - accuracy: 0.9939 - loss: 0.0190 - val_accuracy: 0.7999 - val_loss: 1.6554\n",
      "Epoch 115/115\n",
      "\u001b[1m1439/1439\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 139ms/step - accuracy: 0.9921 - loss: 0.0232 - val_accuracy: 0.7975 - val_loss: 1.8069\n"
     ]
    }
   ],
   "source": [
    "history = model_cnn.fit(X_train, y_train,\n",
    "                    batch_size=70,  #Se han probado diferentes, este parece que tiene buena relacion tiempo/calidad\n",
    "                    epochs=115,  \n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val),  \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 horas y 8 minutos de entrenamiento.  \n",
    ">Obviamente depender√° del ordenador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EVALUACION CONTRA EL TEST** \n",
    ">**El modelo, en su entrenamiento, no ha visto estos ni de refilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss en el conjunto de prueba:: 1.5009\n",
      "Accuracy en el conjunto de prueba:: 81.17%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Loss en el conjunto de prueba:: {loss:.4f}')\n",
    "print(f'Accuracy en el conjunto de prueba:: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss en el conjunto de prueba: 1.5009  \n",
    "Accuracy en el conjunto de prueba:: 81.17%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **En un principio, una precisi√≥n del 81.17% puede parecer relativamente baja, pero es un resultado aceptable para el reconocimiento de expresiones faciales, especialmente considerando el conjunto de datos FER2013 utilizado. El Ministerio de Bienestar Emocional deber√≠a estar satisfecho con este desempe√±o inicial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREDICCIONES CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m394/394\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_cnn.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CLASSIFICATION REPORT CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Enfado       0.80      0.73      0.76      1797\n",
      "        Asco       0.98      0.99      0.98      1798\n",
      "       Miedo       0.71      0.71      0.71      1798\n",
      "   Felicidad       0.86      0.88      0.87      1798\n",
      "    Tristeza       0.70      0.71      0.71      1798\n",
      "    Sorpresa       0.90      0.91      0.90      1798\n",
      "     Neutral       0.73      0.76      0.74      1798\n",
      "\n",
      "    accuracy                           0.81     12585\n",
      "   macro avg       0.81      0.81      0.81     12585\n",
      "weighted avg       0.81      0.81      0.81     12585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Enfado', 'Asco', 'Miedo', 'Felicidad', 'Tristeza', 'Sorpresa', 'Neutral']\n",
    "print(classification_report(y_test, y_pred_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GUARDADO DEL MODELO CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_dir = 'modelos_entrenados'\n",
    "\n",
    "model_cnn.save(os.path.join(save_dir, 'modelo_cnn.h5'))\n",
    "\n",
    "save_model(model_cnn, os.path.join(save_dir, 'modelo_cnn.keras'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **El modelo entrenado ha sido guardado en dos formatos diferentes. Aunque ambos, el formato h5 y el formato keras, mantienen la misma calidad de los datos, la elecci√≥n del formato puede afectar su usabilidad a largo plazo. Por ello, hemos optado por ofrecer ambas opciones al cliente para que pueda seleccionar la m√°s conveniente seg√∫n sus necesidades futuras.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESUMEN/EXPLICACI√ìN DEL MODELO CNN**\n",
    "\n",
    "Este modelo CNN (Convolutional Neural Network) est√° dise√±ado para reconocer las expresiones faciales en fotografias en blanco y negro de 48x48 p√≠xeles, segun \n",
    "chat gtp son algo as√≠ como un cerebro digital que aprende a identificar emociones (realmente expresiones faciales) en fotos. üß†\n",
    "\n",
    "- **Capas Convolucionales**: Son como filtros que detectan caracter√≠sticas como bordes y texturas en las im√°genes  \n",
    "propias de cada expresion, es decir busca las caracteriasticas comunes a cada una de las 7 expresiones faciales \n",
    "que este modelo reconoce, otorgandole una etiqueta a cada una.    \n",
    "\n",
    "- **Capas Densas**: Son como capas finales que interpretan estas caracter√≠sticas para decidir qu√© emoci√≥n se ve en la imagen.    \n",
    "Digamos que es como un inspector de policia que ve las pruebas (las caracteristicas) y dice \"El sujeto est√° feliz\"  \n",
    "  \n",
    "- **Dropout**: Es una t√©cnica para evitar que el modelo memorice demasiado y pueda generalizar mejor.    \n",
    "¬øConoces esa fotografia de un hombre durmiendo en un colch√≥n que tiene su forma? pues con dropout se busca evitar eso.  \n",
    "  \n",
    "- **Funci√≥n de P√©rdida**: Durante el entrenamiento (usando la parte de datos reservada para validaci√≥n) comprueba como   \n",
    "de grande son los fallos que est√° teniendo.    \n",
    "\n",
    "- **Compilaci√≥n**:  Es como organizar la granja por la ma√±ana. Preparamos al modelo dici√©ndole qu√© errores evitar,    \n",
    "c√≥mo ajustar los pesos de las neuronas (con Adam, ahora lo vemos), y qu√© m√©tricas usar para ver como de bien est√° aprendiendo.  \n",
    "  \n",
    "- **Optimizador (Adam)**:  El optimizador Adam ajusta c√≥mo la CNN aprende de los datos. Es como un \"entrenador personal\" (O el jefe de la granja)    \n",
    "que gu√≠a al modelo sobre c√≥mo mejorar durante el entrenamiento, ajustando los pesos y sesgos de las neuronas para que pueda    \n",
    "predecir mejor las expresiones faciales en las im√°genes.    \n",
    "  \n",
    "- **Callback (EarlyStopping)**: Es como \"el capataz\" durante el entrenamiento. Monitorea como la precisi√≥n en los datos de validaci√≥n    \n",
    "va mejorando y detiene el entrenamiento cuando la precisi√≥n deja de mejorar, restaurando los mejores pesos del modelo.    \n",
    "Esto ayuda a prevenir el sobreajuste y asegura que el modelo generalize bien a nuevos datos.    \n",
    "\n",
    "- **M√©tricas**: Digamos que son como las notas del cole, debemos saber como de bueno es el modelo, para ello  \n",
    "dejamos reservada parte, el test, que no veria en el entrenamiento, para poder ver como predice.  \n",
    "En este caso tenemos dos de las asignaturas mas importantes, **Acuracy** (punteria), o sea... del total de prediciones cuantas  \n",
    "eran buenas, poco mas que decir y **loss**, que es como decir \"como de malas son las prediciones que son incorrectas\" dicho de otro modo, En un modelo que diferenciara aniamales, si se predice \"Gorila\" y la foto era un chimpanc√© la prediccion, sin ser correcta, no es lo peor del mundo, en cambio si predigera \"berberecho\" estar√≠a muy mal.\n",
    "\n",
    "Entrenamos el modelo con los datos de entrenamiento y luego lo evaluamos para ver si puede predecir emociones en datos que no ha visto.  \n",
    "Segun chatgtp (y cito textualmente) \"¬°Es como ense√±ar a una computadora a leer expresiones faciales! üòä\"\n",
    "\n",
    "\n",
    "Es importante tener en cuenta que la salida del modelo no son strings, no nos dice directamente la emoci√≥n, nos da un numero entero, \n",
    "por lo que usar un diccionario se hace neecsario, al menos conocerlos: \n",
    "  \n",
    "- 0 para 'Enfado' üò°\n",
    "- 1 para 'Asco' ü§¢\n",
    "- 2 para 'Miedo' üò®\n",
    "- 3 para 'Felicidad' üòÑ\n",
    "- 4 para 'Tristeza' üò¢\n",
    "- 5 para 'Sorpresa' üòÆ\n",
    "- 6 para 'Neutral' üòê\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imagenes/VGG16.jpg\" alt=\"Imagen creada con inteligencia artificial y editada con Microsoft Paint\" style=\"border-radius: 15px; width: 95%;\">\n",
    "\n",
    "\n",
    "\n",
    "*Imagen creada con inteligencia artificial*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEFINICI√ìN DEL MODELO BASE**  \n",
    ">**aqu√≠ se define el modelo preentrenado VGG-16, utilizando pesos preentrenados de 'imagenet'**   \n",
    "**y configurando la entrada para im√°genes de 48x48 p√≠xeles con 3 canales de color.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A√ëADIDO DE CAPAS**  \n",
    ">**Con estas l√≠neas adaptamos el modelo preentrenado VGG16 para que pueda hacer clasificaciones de**    \n",
    "**expresiones faciales en nuestro caso, teniendo como entrada im√°genes de 48x48 en escala de grises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pepea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_vgg = models.Sequential()\n",
    "model_vgg.add(layers.Conv2D(3, (1, 1), input_shape=(48, 48, 1)))  # Convertimos a 3 canales (o sea, colores... gui√±o gui√±o)\n",
    "model_vgg.add(base_model)\n",
    "model_vgg.add(layers.Flatten())\n",
    "model_vgg.add(layers.Dense(4096, activation='relu'))\n",
    "model_vgg.add(layers.Dense(4096, activation='relu'))\n",
    "model_vgg.add(layers.Dense(7, activation='softmax'))  # 7 expresiones faciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONGELADO DE CAPAS**  \n",
    ">**Congelar las capas del modelo base (VGG-16) significa mantener sus pesos preentrenados,**    \n",
    "**lo que aprovecha el conocimiento aprendido en im√°genes generales. Esto acelera el entrenamient**  \n",
    "**y reduce el riesgo de sobreajuste al adaptar el modelo para clasificar expresiones faciales en**    \n",
    "**im√°genes de 48x48 p√≠xeles en escala de grises.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COMPILADO DEL MODELO VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.compile(optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ENTRENAMIENTO DEL MODELO VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1298s\u001b[0m 836ms/step - accuracy: 0.3686 - loss: 1.6483 - val_accuracy: 0.4880 - val_loss: 1.3291\n",
      "Epoch 2/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 835ms/step - accuracy: 0.5264 - loss: 1.2363 - val_accuracy: 0.5594 - val_loss: 1.1594\n",
      "Epoch 3/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.6132 - loss: 1.0189 - val_accuracy: 0.6048 - val_loss: 1.0534\n",
      "Epoch 4/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.7059 - loss: 0.7962 - val_accuracy: 0.6714 - val_loss: 0.8967\n",
      "Epoch 5/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 836ms/step - accuracy: 0.7921 - loss: 0.5774 - val_accuracy: 0.7242 - val_loss: 0.7895\n",
      "Epoch 6/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.8613 - loss: 0.3987 - val_accuracy: 0.7492 - val_loss: 0.7395\n",
      "Epoch 7/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 834ms/step - accuracy: 0.9116 - loss: 0.2671 - val_accuracy: 0.7778 - val_loss: 0.6931\n",
      "Epoch 8/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 835ms/step - accuracy: 0.9443 - loss: 0.1828 - val_accuracy: 0.8027 - val_loss: 0.6573\n",
      "Epoch 9/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 836ms/step - accuracy: 0.9601 - loss: 0.1332 - val_accuracy: 0.7960 - val_loss: 0.7144\n",
      "Epoch 10/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 834ms/step - accuracy: 0.9679 - loss: 0.1105 - val_accuracy: 0.8110 - val_loss: 0.6763\n",
      "Epoch 11/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1292s\u001b[0m 834ms/step - accuracy: 0.9724 - loss: 0.0927 - val_accuracy: 0.8087 - val_loss: 0.7282\n",
      "Epoch 12/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1297s\u001b[0m 837ms/step - accuracy: 0.9744 - loss: 0.0859 - val_accuracy: 0.8203 - val_loss: 0.6823\n",
      "Epoch 13/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1441s\u001b[0m 930ms/step - accuracy: 0.9786 - loss: 0.0732 - val_accuracy: 0.8261 - val_loss: 0.6919\n",
      "Epoch 14/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1222s\u001b[0m 789ms/step - accuracy: 0.9790 - loss: 0.0711 - val_accuracy: 0.8243 - val_loss: 0.7208\n",
      "Epoch 15/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1111s\u001b[0m 717ms/step - accuracy: 0.9816 - loss: 0.0616 - val_accuracy: 0.8326 - val_loss: 0.6980\n",
      "Epoch 16/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1093s\u001b[0m 706ms/step - accuracy: 0.9822 - loss: 0.0576 - val_accuracy: 0.8312 - val_loss: 0.7135\n",
      "Epoch 17/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1111s\u001b[0m 717ms/step - accuracy: 0.9843 - loss: 0.0522 - val_accuracy: 0.8264 - val_loss: 0.7793\n",
      "Epoch 18/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1148s\u001b[0m 741ms/step - accuracy: 0.9830 - loss: 0.0566 - val_accuracy: 0.8322 - val_loss: 0.7674\n",
      "Epoch 19/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1163s\u001b[0m 751ms/step - accuracy: 0.9832 - loss: 0.0551 - val_accuracy: 0.8434 - val_loss: 0.6862\n",
      "Epoch 20/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1137s\u001b[0m 734ms/step - accuracy: 0.9845 - loss: 0.0498 - val_accuracy: 0.8439 - val_loss: 0.7552\n",
      "Epoch 21/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1137s\u001b[0m 734ms/step - accuracy: 0.9840 - loss: 0.0520 - val_accuracy: 0.8261 - val_loss: 0.8703\n",
      "Epoch 22/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1136s\u001b[0m 734ms/step - accuracy: 0.9848 - loss: 0.0491 - val_accuracy: 0.8373 - val_loss: 0.7880\n",
      "Epoch 23/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1119s\u001b[0m 723ms/step - accuracy: 0.9846 - loss: 0.0498 - val_accuracy: 0.8451 - val_loss: 0.7631\n",
      "Epoch 24/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1123s\u001b[0m 725ms/step - accuracy: 0.9842 - loss: 0.0516 - val_accuracy: 0.8485 - val_loss: 0.7386\n",
      "Epoch 25/25\n",
      "\u001b[1m1549/1549\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1123s\u001b[0m 725ms/step - accuracy: 0.9883 - loss: 0.0385 - val_accuracy: 0.8393 - val_loss: 0.8001\n"
     ]
    }
   ],
   "source": [
    "history = model_vgg.fit(X_train, y_train, epochs=25, batch_size=65, \n",
    "                        validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 HORAS Y 30 MINUTOS DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EVALUACION CONTRA EL TEST** \n",
    ">**El modelo, en su entrenamiento, no ha visto estos ni de refilon (si, otra vez)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss en el conjunto de prueba: 0.7857\n",
      "Accuracy en el conjunto de prueba: 84.41%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_vgg.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Loss en el conjunto de prueba: {loss:.4f}')\n",
    "print(f'Accuracy en el conjunto de prueba: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss en el conjunto de prueba: 0.7857  \n",
    "Accuracy en el conjunto de prueba: 84.41%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREDICCIONES VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m394/394\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_vgg = model_cnn.predict(X_test)\n",
    "y_pred_classes_vgg = np.argmax(y_pred_vgg, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CLASSIFICATION REPORT CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Enfado       0.80      0.73      0.76      1797\n",
      "        Asco       0.98      0.99      0.98      1798\n",
      "       Miedo       0.71      0.71      0.71      1798\n",
      "   Felicidad       0.86      0.88      0.87      1798\n",
      "    Tristeza       0.70      0.71      0.71      1798\n",
      "    Sorpresa       0.90      0.91      0.90      1798\n",
      "     Neutral       0.73      0.76      0.74      1798\n",
      "\n",
      "    accuracy                           0.81     12585\n",
      "   macro avg       0.81      0.81      0.81     12585\n",
      "weighted avg       0.81      0.81      0.81     12585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Enfado', 'Asco', 'Miedo', 'Felicidad', 'Tristeza', 'Sorpresa', 'Neutral']\n",
    "print(classification_report(y_test, y_pred_classes_vgg, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GUARDADO DEL MODELO VGG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_vgg.save(os.path.join(save_dir, 'modelo_vgg.h5'))\n",
    "\n",
    "save_model(model_vgg, os.path.join(save_dir, 'modelo_vgg.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESUMEN/EXPLICACI√ìN DEL MODELO VGG16**\n",
    "\n",
    "Este modelo VGG16 modificado est√° dise√±ado para reconocer expresiones faciales en fotograf√≠as en blanco y negro de 48x48 p√≠xeles, aunque es necesario convertirlas a 3 canales (RGB) para poder utilizar el modelo base VGG16 preentrenado en ImageNet.\n",
    "\n",
    "> ü§´ **Nota**: \"RGB\" significa \"Red, Green, Blue\" en ingl√©s, que son los colores primarios usados en pantallas para mostrar colores.\n",
    "\n",
    "### **Capas Convolucionales (VGG16):**\n",
    "\n",
    "Utilizaremos el **modelo VGG16** como base, el cual ha sido previamente entrenado con un gran conjunto de datos generales.  \n",
    "Aprovecharemos los pesos y conocimientos adquiridos durante su entrenamiento, que consiste en m√∫ltiples capas convolucionales  \n",
    "y de agrupaci√≥n para detectar caracter√≠sticas complejas como bordes, texturas y patrones en las im√°genes.  \n",
    "\n",
    "### **Capas Densas Personalizadas:**\n",
    "\n",
    "Se a√±aden capas densas para interpretar las caracter√≠sticas extra√≠das por VGG16 y clasificar las im√°genes en una de las 7 expresiones faciales.    \n",
    "Estas capas densas act√∫an como \"inspectores\" que determinan qu√© emoci√≥n se observa en la imagen.  \n",
    "\n",
    "### **Congelaci√≥n de Capas:**\n",
    "\n",
    "Todas las capas de VGG16 se congelan para mantener los pesos preentrenados obtenidos de ImageNet. Esto aprovecha el conocimiento previo      \n",
    "del modelo base y evita que se sobrescriban durante el entrenamiento con nuevos datos.  \n",
    "\n",
    "### **Compilaci√≥n del Modelo VGG16:**\n",
    "\n",
    "El modelo se compila utilizando el optimizador Adam con una tasa de aprendizaje de 0.0005 y la funci√≥n de p√©rdida **'sparse_categorical_crossentropy'**.    \n",
    "Esto prepara al modelo para el entrenamiento, especificando c√≥mo debe actualizarse durante el proceso de aprendizaje.  \n",
    "\n",
    "> **Compilar** en el contexto de la programaci√≥n y el aprendizaje autom√°tico significa preparar un programa o modelo para su ejecuci√≥n o entrenamiento,  \n",
    "optimizando el c√≥digo fuente y las operaciones para la m√°quina.  \n",
    ">\n",
    "> **'sparse_categorical_crossentropy'** es una funci√≥n de p√©rdida utilizada en problemas de clasificaci√≥n donde las etiquetas son n√∫meros enteros (sparse),  \n",
    "es decir, no est√°n codificadas en formato one-hot. Calcula la discrepancia entre las etiquetas reales y las predicciones del modelo para mejorar su  \n",
    "capacidad de clasificaci√≥n.\n",
    "\n",
    "### **Entrenamiento del Modelo:**\n",
    "\n",
    "El modelo se entrena durante 25 √©pocas utilizando datos de entrenamiento con lotes de 65 im√°genes cada uno. Se utiliza un conjunto de validaci√≥n  \n",
    "para monitorear el rendimiento y evitar el sobreajuste.  \n",
    "\n",
    "### **Evaluaci√≥n del Modelo:**\n",
    "\n",
    "Despu√©s del entrenamiento, se eval√∫a el modelo utilizando un conjunto de prueba reservado previamente. Se calcula la precisi√≥n (accuracy) y otras  \n",
    "m√©tricas para comprender qu√© tan bien generaliza el modelo con nuevos datos.  \n",
    "\n",
    "### **Guardado del Modelo:**\n",
    "\n",
    "Hacemos lo mismo que con el modelo anterior, obviamente cambiando los nombres.  \n",
    "\n",
    "En este caso, la salida del modelo tambi√©n es num√©rica, por lo que vuelve a ser necesario un diccionario para interpretar las predicciones. El diccionario es el mismo.\n",
    "- 0 para 'Enfado' üò°\n",
    "- 1 para 'Asco' ü§¢\n",
    "- 2 para 'Miedo' üò®\n",
    "- 3 para 'Felicidad' üòÑ\n",
    "- 4 para 'Tristeza' üò¢\n",
    "- 5 para 'Sorpresa' üòÆ\n",
    "- 6 para 'Neutral' üòê\n",
    "\n",
    "**ChatGPT dice**: ¬°El modelo VGG16 es una poderosa herramienta para el reconocimiento de expresiones faciales, adaptada y optimizada para este desaf√≠o espec√≠fico!  \n",
    "\n",
    "## **¬øPOR QU√â USAR DOS MODELOS?** \n",
    "\n",
    "Reconocer expresiones faciales es una tarea relativamente simple para los seres humanos, pero para los modelos de aprendizaje autom√°tico no es tan f√°cil. Interpretar y distinguir diferencias sutiles entre expresiones emocionales puede ser complicado para un solo modelo. Por lo tanto, para aumentar la confianza en las predicciones, se han entrenado dos modelos.\n",
    "\n",
    "Ambos modelos est√°n entrenados con el mismo conjunto de datos (Fer2013, acuerdate) y son bastante similares en su arquitectura. Sin embargo, el programa realiza predicciones con ambos modelos y selecciona la predicci√≥n que tenga el mayor nivel de confianza. En casos donde ambos modelos predicen diferentes emociones pero con igual nivel de confianza, se requerir√° una revisi√≥n humana para tomar la decisi√≥n final.\n",
    "\n",
    "Para mitigar el sesgo y sobreajuste debido a la utilizaci√≥n del mismo conjunto de datos, ambos modelos ser√°n reentrenados con un nuevo dataset. Esto permitir√° que los modelos generalicen mejor las predicciones a nuevas im√°genes y situaciones.\n",
    "\n",
    "Este enfoque de utilizar dos modelos y validar las predicciones con criterios de confianza y revisi√≥n humana ayuda a mejorar la precisi√≥n y fiabilidad del sistema de reconocimiento de expresiones faciales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
